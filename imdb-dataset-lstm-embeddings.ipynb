{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3176,"sourceType":"datasetVersion","datasetId":1835},{"sourceId":11683,"sourceType":"datasetVersion","datasetId":8352},{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentence-Level Categorization on TREC Dataset","metadata":{}},{"cell_type":"code","source":"#--- Imports ---#\n\n# Setting Random Seed.\nimport os\nimport random\nimport tensorflow as tf\n\n# Data Preprocessing.\nimport codecs # For reading FastText Embedding file.\nimport numpy  as np\nimport pandas as pd\nfrom tqdm import tqdm # For tracking progress of iterable processes.\nfrom sklearn.model_selection  import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\n\n# Model Architecture.\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, Bidirectional, LSTM, \\\n                         BatchNormalization, Dropout, \\\n                         GlobalMaxPooling1D, GlobalAveragePooling1D, Dense\nfrom keras.utils import plot_model\nfrom IPython.display import Image\n\n# Model Training.\nfrom time import time\nfrom keras.callbacks import Callback, LearningRateScheduler, EarlyStopping\n\n# Data Visualization.\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-08T14:28:34.647183Z","iopub.execute_input":"2023-11-08T14:28:34.647488Z","iopub.status.idle":"2023-11-08T14:28:43.913940Z","shell.execute_reply.started":"2023-11-08T14:28:34.647461Z","shell.execute_reply":"2023-11-08T14:28:43.912800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Set Random Seed ---#\n\n# Create the seed.\nWORD = \"meow\"                          # Set a string to use as the seed.\nseed = sum(ord(char) for char in WORD) # Convert `WORD` to numerical representation.\n\n# Set the seed.\nrandom.seed(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:28:43.915841Z","iopub.execute_input":"2023-11-08T14:28:43.916469Z","iopub.status.idle":"2023-11-08T14:28:43.923768Z","shell.execute_reply.started":"2023-11-08T14:28:43.916433Z","shell.execute_reply":"2023-11-08T14:28:43.922562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Constants ---#\n\nRAND_STATE = seed   # Random State.\nTRAIN_SIZE = 0.6    # Size of Train Dataset.\nVAL_SIZE   = 0.15   # Size of Validation Dataset.\nTEST_SIZE  = 0.15   # Size of Test Dataset.\n\nEMBED_DIM  = 100    # Set Embedding Dimensions.\nLSTM_CELLS = 128    # Number of LSTM cells for LSTM layer.\nDROPOUT    = 0.5    # Dropout Rate for Dropout Layer.\nDENSE_DIM  = 128    # Dimensions of Dense Layer.\n\nLEARN_RATE = 0.001  # Initial Learning Rate.\nLR_DECAY   = 0.95   # Learning Rate Decay.\nPATIENCE   = 10     # Early Stopping Patience.\nEPOCHS     = 10000  # Number of training epochs.\nBATCH_SIZE = 32     # Training Batch Size.","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:28:43.925431Z","iopub.execute_input":"2023-11-08T14:28:43.925847Z","iopub.status.idle":"2023-11-08T14:28:43.934404Z","shell.execute_reply.started":"2023-11-08T14:28:43.925749Z","shell.execute_reply":"2023-11-08T14:28:43.933408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Load FastText Embeddings ---#\n\n# Create Embedding Dictionary.\ndict_fasttext_embedding = {} \n\n# Open FastText Embedding file.\nf = codecs.open('../input/fasttext/wiki.simple.vec', encoding='utf-8')\n\n# Populate Embedding Dictionary.\nfor line in tqdm(f):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    dict_fasttext_embedding[word] = coefs\n    \nf.close()\n\n# Get FastText Embedding Vocabulary Size.\nvocab_size_fasttext = len(dict_fasttext_embedding)\nprint(f\"FastText Embedding Vocabulary Size: {vocab_size_fasttext}\")\n\n# Get FastText Embedding Dimensions.\nembed_dim_fasttext = dict_fasttext_embedding['hello'].shape[0]\nprint(f\"FastText Embedding Dimensions: {embed_dim_fasttext}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:28:43.937145Z","iopub.execute_input":"2023-11-08T14:28:43.937450Z","iopub.status.idle":"2023-11-08T14:29:00.255301Z","shell.execute_reply.started":"2023-11-08T14:28:43.937423Z","shell.execute_reply":"2023-11-08T14:29:00.253992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Load GloVe Embeddings ---#\n\n# Read GloVe Embedding CSV file.\nglove = pd.read_csv('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt', \n                    sep=\" \", quoting=3, header=None, index_col=0)\n\n# Create Embedding Dictionary and populate with GloVe Embeddings.\ndict_glove_embedding = {key: val.values for key, val in glove.T.items()}\n\n# Get FastText Embedding Vocabulary Size.\nvocab_size_glove = len(dict_glove_embedding)\nprint(f\"GloVe Embedding Vocabulary Size: {vocab_size_glove}\")\n\n# Get FastText Embedding Dimensions.\nembed_dim_glove = dict_glove_embedding['hello'].shape[0]\nprint(f\"GloVe Embedding Dimensions: {embed_dim_glove}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:00.256769Z","iopub.execute_input":"2023-11-08T14:29:00.257189Z","iopub.status.idle":"2023-11-08T14:29:23.120876Z","shell.execute_reply.started":"2023-11-08T14:29:00.257150Z","shell.execute_reply":"2023-11-08T14:29:23.119954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"#--- Read data ---#\n\ndf = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:23.122500Z","iopub.execute_input":"2023-11-08T14:29:23.122811Z","iopub.status.idle":"2023-11-08T14:29:24.595490Z","shell.execute_reply.started":"2023-11-08T14:29:23.122775Z","shell.execute_reply":"2023-11-08T14:29:24.594313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Label Encoding ---#\n\ndf.sentiment = df.sentiment.apply(lambda x: 1 if x=='positive' else 0)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:24.596986Z","iopub.execute_input":"2023-11-08T14:29:24.597351Z","iopub.status.idle":"2023-11-08T14:29:24.647828Z","shell.execute_reply.started":"2023-11-08T14:29:24.597315Z","shell.execute_reply":"2023-11-08T14:29:24.646923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Tokenization ---#\n\n# Get the review text.\nX_text = df['review']\n\n# Get the labels.\ny = df['sentiment']\n\n# Fit tokenizer on review text.\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_text)\n\n# Tokenize review text.\nX_tokenized = tokenizer.texts_to_sequences(X_text)\n\n# Print Dimensions of Embedded Data.\ndims_embedded_data = len(X_tokenized), len(X_tokenized[0])\nprint(f\"Dimensions of Embedded Data: {dims_embedded_data}\")\n\n# Get Tokenizer Vocabulary Size.\nvocab_size_tokenizer = len(tokenizer.word_index) + 1\nprint(f\"Tokenizer Vocabulary Size: {vocab_size_tokenizer}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:24.649027Z","iopub.execute_input":"2023-11-08T14:29:24.649373Z","iopub.status.idle":"2023-11-08T14:29:42.413112Z","shell.execute_reply.started":"2023-11-08T14:29:24.649346Z","shell.execute_reply":"2023-11-08T14:29:42.412097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Plot Sequence Lengths ---#\n\nsequence_lengths = [len(seq) for seq in X_tokenized]\nplt.hist(sequence_lengths, bins=30)\nplt.xlabel('Sequence Length')\nplt.ylabel('Count')\nplt.show()\n\nprint(f\"Mean sequence length: {np.mean(sequence_lengths)}\")\nprint(f\"Median sequence length: {np.median(sequence_lengths)}\")\nprint(f\"Max sequence length: {max(sequence_lengths)}\")\nprint(f\"95th percentile sequence length: {np.percentile(sequence_lengths, 95)}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:42.414331Z","iopub.execute_input":"2023-11-08T14:29:42.415046Z","iopub.status.idle":"2023-11-08T14:29:42.969582Z","shell.execute_reply.started":"2023-11-08T14:29:42.415009Z","shell.execute_reply":"2023-11-08T14:29:42.968630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Sequence Padding ---#\n\n# Set sequence length to 95th percentile sequence length.\nsequence_length = int(np.percentile(sequence_lengths, 95))\n\n# Pad and truncate sequences in Embedded Data.\nX_padded = pad_sequences(X_tokenized, maxlen=sequence_length)\n\n# Convert to float32 array.\nX_array = np.asarray(X_padded).astype('float32')\n\n# Print Dimensions of Padded Data.\nprint(f\"Dimensions of Padded Data: {X_array.shape}\")\n\n# Print Sequences Length.\nprint(f\"Sequence Length: {sequence_length}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:42.973023Z","iopub.execute_input":"2023-11-08T14:29:42.973387Z","iopub.status.idle":"2023-11-08T14:29:44.207795Z","shell.execute_reply.started":"2023-11-08T14:29:42.973361Z","shell.execute_reply":"2023-11-08T14:29:44.206721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Data Splitting ---#\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_array,\n                                                  y,\n                                                  test_size=(1 - TRAIN_SIZE),\n                                                  random_state=RAND_STATE)\n\nX_val, X_test, Y_val, Y_test = train_test_split(X_val,\n                                                Y_val,\n                                                test_size=0.5,\n                                                random_state=RAND_STATE)\n\nprint(f\"Shape of X_train: {X_train.shape}\")\nprint(f\"Shape of Y_train: {Y_train.shape}\")\nprint(f\"Shape of X_val: {X_val.shape}\")\nprint(f\"Shape of Y_val: {Y_val.shape}\")\nprint(f\"Shape of X_test: {X_test.shape}\")\nprint(f\"Shape of Y_test: {Y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:44.208826Z","iopub.execute_input":"2023-11-08T14:29:44.209127Z","iopub.status.idle":"2023-11-08T14:29:44.267169Z","shell.execute_reply.started":"2023-11-08T14:29:44.209102Z","shell.execute_reply":"2023-11-08T14:29:44.266136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Definition","metadata":{}},{"cell_type":"code","source":"#--- Function for Creating Embedding Matrix ---#\n\ndef create_embedding_matrix(tokenizer, embed_dim, embedding_dict):\n    \n    # Get Tokenizer Vocabulary Size.\n    vocab_size_tokenizer = len(tokenizer.word_index) + 1\n    \n    # Initialize Embedding Matrix with zeroes.\n    embedding_matrix = np.zeros((vocab_size_tokenizer, embed_dim)) \n    \n    # Populate Embedding Matrix.\n    for word, index in tokenizer.word_index.items():\n        if word in embedding_dict:\n            embedding_matrix[index] = embedding_dict[word]\n            \n    return embedding_matrix","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:44.268285Z","iopub.execute_input":"2023-11-08T14:29:44.268584Z","iopub.status.idle":"2023-11-08T14:29:44.274212Z","shell.execute_reply.started":"2023-11-08T14:29:44.268558Z","shell.execute_reply":"2023-11-08T14:29:44.273190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Creating Embedding Matrix ---#\n\n# Create FastText Embedding Matrix.\nembedding_matrix_fasttext = create_embedding_matrix(tokenizer, \n                                                    embed_dim_fasttext, \n                                                    dict_fasttext_embedding)\n\n# Create GloVe Embedding Matrix.\nembedding_matrix_glove = create_embedding_matrix(tokenizer, \n                                                 embed_dim_glove, \n                                                 dict_glove_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:44.275546Z","iopub.execute_input":"2023-11-08T14:29:44.275807Z","iopub.status.idle":"2023-11-08T14:29:44.756801Z","shell.execute_reply.started":"2023-11-08T14:29:44.275784Z","shell.execute_reply":"2023-11-08T14:29:44.755948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Model Definition ---#\n\ndef create_model(vocab_size_tokenizer, \n                 sequence_length, \n                 embed_dim,\n                 embedding_matrix=None,\n                 aggregation='max'):\n            \n    input_layer = Input(shape=(sequence_length,))\n\n    if embedding_matrix is None:\n        embedding_layer = Embedding(input_dim=vocab_size_tokenizer, \n                                    output_dim=embed_dim, \n                                    input_length=sequence_length\n                                   )(input_layer)\n    else:\n        embedding_layer = Embedding(input_dim=vocab_size_tokenizer,\n                                    output_dim=embed_dim,  \n                                    input_length=sequence_length, \n                                    weights=[embedding_matrix],   \n                                    trainable=False      \n                                   )(input_layer)\n\n    lstm_layer        = Bidirectional(LSTM(int(LSTM_CELLS*1.0), return_sequences=False))(embedding_layer)\n\n    dropout_layer     = Dropout(rate=DROPOUT)(lstm_layer)\n\n    dense_layer       = Dense(int(DENSE_DIM*1.0), activation='relu')(dropout_layer)\n\n    output_layer      = Dense(1, activation='sigmoid')(dense_layer)\n\n    model = Model(inputs=input_layer, outputs=output_layer)\n    \n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])\n    \n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:44.758021Z","iopub.execute_input":"2023-11-08T14:29:44.758314Z","iopub.status.idle":"2023-11-08T14:29:44.767540Z","shell.execute_reply.started":"2023-11-08T14:29:44.758289Z","shell.execute_reply":"2023-11-08T14:29:44.766039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Create Model without using Pretrained Embedding ---#\n\nmodel_noembedding = create_model(vocab_size_tokenizer, \n                                 sequence_length,\n                                 EMBED_DIM,\n                                 embedding_matrix=None,\n                                 agbgregation='max')\n\nplot_model(model_noembedding,\n           to_file='model_noembedding.png',\n           show_shapes=True,\n           show_layer_names=True,\n           show_layer_activations=True,\n           show_trainable=True)\n\nprint(model_noembedding.summary())\n\nImage(filename='model_noembedding.png')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:44.768812Z","iopub.execute_input":"2023-11-08T14:29:44.769188Z","iopub.status.idle":"2023-11-08T14:29:48.711573Z","shell.execute_reply.started":"2023-11-08T14:29:44.769144Z","shell.execute_reply":"2023-11-08T14:29:48.710736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Create Model using FastText Embedding ---#\n\nmodel_fasttext = create_model(vocab_size_tokenizer, \n                              sequence_length,\n                              embed_dim_fasttext,\n                              embedding_matrix=embedding_matrix_fasttext,\n                              aggregation='max')\n\nplot_model(model_fasttext, \n           to_file='model_fasttext.png', \n           show_shapes=True,\n           show_layer_names=True,\n           show_layer_activations=True,\n           show_trainable=True)\n\nprint(model_fasttext.summary())\n\nImage(filename='model_fasttext.png')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T16:33:07.537712Z","iopub.execute_input":"2023-11-08T16:33:07.538151Z","iopub.status.idle":"2023-11-08T16:33:08.788039Z","shell.execute_reply.started":"2023-11-08T16:33:07.538118Z","shell.execute_reply":"2023-11-08T16:33:08.787111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Create Model using GloVe Embedding ---#\n\nmodel_glove = create_model(vocab_size_tokenizer, \n                           sequence_length,\n                           embed_dim_glove,\n                           embedding_matrix=embedding_matrix_glove,\n                           aggregation='max')\n\nplot_model(model_glove, \n           to_file='model_glove.png', \n           show_shapes=True,\n           show_layer_names=True,\n           show_layer_activations=True,\n           show_trainable=True)\n\nprint(model_glove.summary())\n\nImage(filename='model_glove.png')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T16:33:08.789510Z","iopub.execute_input":"2023-11-08T16:33:08.789806Z","iopub.status.idle":"2023-11-08T16:33:09.496785Z","shell.execute_reply.started":"2023-11-08T16:33:08.789781Z","shell.execute_reply":"2023-11-08T16:33:09.495789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"#--- Callbacks ---#\n\n#--- Callback Class for Saving Model Weights ---#\n\nclass SaveWeights(Callback):\n    \n    def __init__(self):\n        super(SaveWeights, self).__init__()\n        self.weights_history = []\n\n    def on_epoch_end(self, epoch, logs=None):\n        # Get weights of first layer.\n        layer_weights = self.model.layers[-1].get_weights() \n        self.weights_history.append(layer_weights)\n        \n#--- Callback Class for Training Time History ---#\n\nclass TimeHistory(Callback):\n    \n    def on_train_begin(self, logs={}):\n        self.time_start = time()        # Track time taken for each epoch.\n        self.times = []                 # Track total time taken for training.\n\n    def on_epoch_begin(self, batch, logs={}):\n        self.epoch_time_start = time()\n\n    def on_epoch_end(self, batch, logs={}):\n        self.times.append(time() - self.epoch_time_start)\n        \n    def on_train_end(self, logs={}):\n        self.time_total = time() - self.time_start\n        \n#--- Callback Function for Learning Rate Scheduling ---#\n\ndef lr_schedule(epoch):\n    \n    initial_lr = 0.01                           # Set the initial learning rate.\n    learning_rate = initial_lr * (0.9 ** epoch) # Update learning rate with decay.\n    \n    return learning_rate","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:50.304395Z","iopub.execute_input":"2023-11-08T14:29:50.304700Z","iopub.status.idle":"2023-11-08T14:29:50.313235Z","shell.execute_reply.started":"2023-11-08T14:29:50.304674Z","shell.execute_reply":"2023-11-08T14:29:50.312418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Model Training ---#\n\ndef train_model(model, X_train, Y_train, X_val, Y_val):\n\n    # Define Callbacks.\n    cb_earlystop    = EarlyStopping(monitor='val_accuracy', patience=PATIENCE, verbose=2)\n    cb_saveweights  = SaveWeights()\n    cb_timehistory  = TimeHistory()\n    cb_lr_scheduler = LearningRateScheduler(lr_schedule)\n\n    callbacks = [cb_earlystop, cb_saveweights, cb_timehistory, cb_lr_scheduler]\n\n    history = model.fit(X_train, \n                        Y_train, \n                        validation_data=(X_val, Y_val),\n                        epochs=EPOCHS, \n                        batch_size=BATCH_SIZE, \n                        callbacks=callbacks,\n                        verbose=2)\n    \n    df_results = pd.DataFrame(history.history)\n\n    df_results['epoch'] = history.epoch\n    df_results['time']  = cb_timehistory.times\n\n    df_weights = pd.DataFrame(cb_saveweights.weights_history)\n    total_time = cb_timehistory.time_total\n\n    results = {'results'    : df_results,\n               'weights'    : df_weights,\n               'total_time' : total_time}\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:50.314351Z","iopub.execute_input":"2023-11-08T14:29:50.314633Z","iopub.status.idle":"2023-11-08T14:29:50.327665Z","shell.execute_reply.started":"2023-11-08T14:29:50.314607Z","shell.execute_reply":"2023-11-08T14:29:50.326646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Train Model without using Pretrained Embedding ---#\n\nresults_noembedding = train_model(model_noembedding, X_train, Y_train, X_val, Y_val)\nprint(f'Training Time: {results_noembedding[\"total_time\"]}')\n\n#--- Predict using Model without using Pretrained Embedding ---#\n\nloss, accuracy = model_noembedding.evaluate(X_test, Y_test)\nprint(f'Test loss: {loss:.4f}')\nprint(f'Test accuracy: {accuracy:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T14:29:50.328764Z","iopub.execute_input":"2023-11-08T14:29:50.329080Z","iopub.status.idle":"2023-11-08T15:01:19.887596Z","shell.execute_reply.started":"2023-11-08T14:29:50.329045Z","shell.execute_reply":"2023-11-08T15:01:19.886612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Train Model using FastText Embedding ---#\n\nresults_fasttext = train_model(model_fasttext, X_train, Y_train, X_val, Y_val)\nprint(f'Training Time: {results_fasttext[\"total_time\"]}')\n\n#--- Predict using Model using FastText Embedding ---#\n\nloss, accuracy = model_fasttext.evaluate(X_test, Y_test)\nprint(f'Test loss: {loss:.4f}')\nprint(f'Test accuracy: {accuracy:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T15:01:19.888794Z","iopub.execute_input":"2023-11-08T15:01:19.889111Z","iopub.status.idle":"2023-11-08T15:20:03.990011Z","shell.execute_reply.started":"2023-11-08T15:01:19.889083Z","shell.execute_reply":"2023-11-08T15:20:03.989182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Train Model using GloVe Embedding ---#\n\nresults_glove = train_model(model_glove, X_train, Y_train, X_val, Y_val)\nprint(f'Training Time: {results_glove[\"total_time\"]}')\n\n#--- Predict using Model using FastText Embedding ---#\n\nloss, accuracy = model_glove.evaluate(X_test, Y_test)\nprint(f'Test loss: {loss:.4f}')\nprint(f'Test accuracy: {accuracy:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T15:20:03.991224Z","iopub.execute_input":"2023-11-08T15:20:03.991525Z","iopub.status.idle":"2023-11-08T15:41:04.012796Z","shell.execute_reply.started":"2023-11-08T15:20:03.991498Z","shell.execute_reply":"2023-11-08T15:41:04.011553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- Saving Models ---#\n\nmodel_noembedding.save('model_noembedding.h5')\nmodel_fasttext.save('model_fasttext.h5')\nmodel_glove.save('model_glove.h5')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T15:41:04.014328Z","iopub.execute_input":"2023-11-08T15:41:04.014716Z","iopub.status.idle":"2023-11-08T15:41:04.817864Z","shell.execute_reply.started":"2023-11-08T15:41:04.014683Z","shell.execute_reply":"2023-11-08T15:41:04.817055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"#--- Plot Accuracy ---#\n\n# Create a figure and axes for the accuracy\nfig, ax1 = plt.subplots(figsize=(10, 6))\n\n# Plot training and validation accuracy on the first y-axis\nax1.plot(results_noembedding['results']['epoch'], results_noembedding['results']['val_accuracy'], \n         label='No Embedding Accuracy',   marker='o', color='darkblue')\nax1.plot(results_fasttext['results']['epoch'], results_fasttext['results']['val_accuracy'], \n         label='FastText Accuracy', marker='o', color='firebrick')\nax1.plot(results_glove['results']['epoch'], results_glove['results']['val_accuracy'], \n         label='GloVe Accuracy', marker='o', color='seagreen')\n\n# Set labels and a legend for the first y-axis\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Accuracy', color='tab:blue')\nax1.legend(loc='center left')\n\n# Create a second y-axis sharing the same x-axis\nax2 = ax1.twinx()\n\n# Plot training and validation loss on the second y-axis\nax2.plot(results_noembedding['results']['epoch'], results_noembedding['results']['val_loss'], \n         label='No Embedding Loss',   marker='o', color='lightskyblue')\nax2.plot(results_fasttext['results']['epoch'], results_fasttext['results']['val_loss'], \n         label='FastText Loss', marker='o', color='lightcoral')\nax2.plot(results_glove['results']['epoch'], results_glove['results']['val_loss'], \n         label='GloVe Loss', marker='o', color='lightgreen')\n\n# Set labels and a legend for the second y-axis\nax2.set_ylabel('Loss', color='tab:red')\nax2.legend(loc='center right')\n\n# Title and overall legend\nplt.title('Training and Validation Accuracy and Loss Over Epochs')\nplt.legend(loc='center right')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T15:41:04.819026Z","iopub.execute_input":"2023-11-08T15:41:04.819324Z","iopub.status.idle":"2023-11-08T15:41:05.245832Z","shell.execute_reply.started":"2023-11-08T15:41:04.819298Z","shell.execute_reply":"2023-11-08T15:41:05.244836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n\nmodel_noembedding = keras.models.load_model('model_noembedding.h5')\n\nloss, accuracy = model_noembedding.evaluate(X_test, Y_test)\nprint(f'Test loss: {loss:.4f}')\nprint(f'Test accuracy: {accuracy:.4f}')\nY_pred = model_noembedding.predict(X_test)\n\npredictions = [1 if i>0.5 else 0  for i in Y_pred ]\n\naccuracy = accuracy_score(Y_test, predictions)\nf1 = f1_score(Y_test, predictions)\nconf_matrix = confusion_matrix(Y_test, predictions)\n\nprint(accuracy)\nprint(f1)\nprint(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T18:06:52.077017Z","iopub.execute_input":"2023-11-08T18:06:52.077863Z","iopub.status.idle":"2023-11-08T18:07:14.353914Z","shell.execute_reply.started":"2023-11-08T18:06:52.077828Z","shell.execute_reply":"2023-11-08T18:07:14.352799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_fasttext = keras.models.load_model('model_fasttext.h5')\n\nloss, accuracy = model_fasttext.evaluate(X_test, Y_test)\nprint(f'Test loss: {loss:.4f}')\nprint(f'Test accuracy: {accuracy:.4f}')\nY_pred = model_fasttext.predict(X_test)\n\npredictions = [1 if i>0.5 else 0  for i in Y_pred ]\n\naccuracy = accuracy_score(Y_test, predictions)\nf1 = f1_score(Y_test, predictions)\nconf_matrix = confusion_matrix(Y_test, predictions)\n\nprint(accuracy)\nprint(f1)\nprint(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T18:07:36.968439Z","iopub.execute_input":"2023-11-08T18:07:36.968801Z","iopub.status.idle":"2023-11-08T18:07:58.361098Z","shell.execute_reply.started":"2023-11-08T18:07:36.968771Z","shell.execute_reply":"2023-11-08T18:07:58.359891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_glove = keras.models.load_model('model_glove.h5')\n\nloss, accuracy = model_glove.evaluate(X_test, Y_test)\nprint(f'Test loss: {loss:.4f}')\nprint(f'Test accuracy: {accuracy:.4f}')\nY_pred = model_glove.predict(X_test)\n\npredictions = [1 if i>0.5 else 0  for i in Y_pred ]\n\naccuracy = accuracy_score(Y_test, predictions)\nf1 = f1_score(Y_test, predictions)\nconf_matrix = confusion_matrix(Y_test, predictions)\n\nprint(accuracy)\nprint(f1)\nprint(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T18:07:58.362775Z","iopub.execute_input":"2023-11-08T18:07:58.364454Z","iopub.status.idle":"2023-11-08T18:08:17.292160Z","shell.execute_reply.started":"2023-11-08T18:07:58.364423Z","shell.execute_reply":"2023-11-08T18:08:17.291094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model_noembedding.predict(X_test)\nplt.hist(np.ravel(predictions), bins=10, density=True, alpha=0.7, color='b')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T17:56:56.247329Z","iopub.execute_input":"2023-11-08T17:56:56.248063Z","iopub.status.idle":"2023-11-08T17:57:04.674802Z","shell.execute_reply.started":"2023-11-08T17:56:56.248026Z","shell.execute_reply":"2023-11-08T17:57:04.673928Z"},"trusted":true},"execution_count":null,"outputs":[]}]}